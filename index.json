[{"authors":null,"categories":null,"content":"Hello there!\nI have an inquisitive mind and I enjoy learning and building software solutions to problems.\nI\u0026rsquo;m fascinated by the fact that we are now able to make machines see, read, learn, and make decisions using the knowledge we feed them.\nIT is a world of boundless creation, here\u0026rsquo;s a site to document some of mine.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1615994337,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Hello there!\nI have an inquisitive mind and I enjoy learning and building software solutions to problems.\nI\u0026rsquo;m fascinated by the fact that we are now able to make machines see, read, learn, and make decisions using the knowledge we feed them.","tags":null,"title":"Karyl Ong Jia Hui","type":"authors"},{"authors":null,"categories":null,"content":"Foreword Single image super-resolution (SISR) aims to recover a high-resolution (HR) image from its low-resolution(LR) observation. There are two major challenges to project namely the constraint on the number of trainable parameters of the model and the long training times due to the large sizes of the HR images.\nThe former constraint restricts the use of GANs, which is known to produce higher fidelity images as additional discriminator network will definitely exceed the number of trainable parameters. Due to limited computational resources, training a SISR model on a shared GPU resource provided by the institution took too much time per training iteration. This problem was alleviated by converting the raw images into subimages and stored in lmdb format, which allowed much faster disk IO. However, it will still take an entire day to train 1 million iterations. This limits the amount of empirical tuning during the model training and thus forces students to seek out the most efficient and effective methods to try.\nMy final model was designed to use a modified version of SRResNet (MSRResNet) trained on a mini-DIV2K dataset with numerous data augmentations such as MixUp training and L1 loss. It achieved a PSNR score on 28.99 on the validation set after 2M iterations.\nProblem Statement The objective of this challenge is to build a high performing light-weight neural network model that can increase the resolution of a single image (by four times) using a given dataset. The evaluation set (80 test images) is released one week prior to submission deadline. The higher the PSNR obtained for the private testset, the higher the grades for this challenge.\nThere are some constraints set for this challenge:\nModel should be trained strictly on the provided mini DIV2K training set. Model should contain fewer than 1,821,085 trainable parameters, which is 120% of the trainable parameters in SRResNet. No external data and pretrained models are allowed. Ensemble of models is not allowed. The deliverables of this challenge include:\nA short report in pdf format of not more than five A4 pages. PSNR of model on validation sets Number of parameters of final model The four times upscaled images of the 80 test LR images. Model checkpoint (weights) of submitted model. Source codes for training and testing the model. Dataset Mini DIV2K The mini dataset provided was extracted from the larger DIV2K dataset, which originally contains 800 HR and corresponding low resolution images for training and 100 HR and LR images for validation. In this project, the mini DIV2K dataset contains 500 HR and LRx4 images for training and 80 pairs for validation and will be hereafter denoted as DIV2K for simplicity.\nDataset Preparation As the DIV2K training dataset contains large 2K images, it takes a long time to load the HR images into memory for training. In order to improve the speed of disk IO during training, the 500 HR images are first cropped into 20,424 of 480x480 subimages before converting into a lmdb dataset (HRsub.lmdb) format. Similarly, the 500 corresponding LR images are also cropped into 20,424 of 120x120 subimages before converting to a lmdb dataset (LRx4sub.lmdb).\nOne thing to note is the disadvantage for converting the images into lmdb format. The two lmdb files required close to 70 GB of disk space as compared to the original 3GB of all the image files. However, it is a necessary tradeoff to train using the lmdb files as the training speed was increased at least 2 folds.\nImplementation Data Augmentation Data augmentation is an important step in many computer vision tasks due to the scarcity of labelled dataset. It helps to “increase” the training set by creating variations of the original images. This expanded dataset alleviates the problem of overﬁtting and allows the model to generalize better.\nStandard geometric manipulation techniques such as paired random crop, random horizontal and vertical flips and random rotations, provided by the baseline BasicSR are applied to the model training.\nInspired by Yoo et al. findings on effective augmentations for Image Super Resolution, augmentations such as RGB permutations and Blending, which performs no structural changes to the images were applied. RGB permutation is the random shuffling of the color channels of the HR and LR images (same permutations), while Blending adds a random constant value to the images. As experimented, these two augmentation improves the PSNR by 0.03 on the validation set. Futhermore, MixUp training was also performed. MixUp is a data augmentation technique that blends two images to generate an unseen training sample. As such pairs of LR and HR images are mixed together using a weighted linear interpolation with a λ value sampled from a beta distribution of α = 0.2 (hyperparameter).\nL1 loss $$\\begin{align} L1 \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N |I_{HR}(i) - I_{SR}(i)| \\end{align}$$\nAs the PSNR metric is highly correlated with the pixel-wise difference, and minimizing the pixel loss directly maximizes the PSNR metric value. It was observed that L1 Loss performs better than MSE Loss in terms of PSNR on validation set, thus it was used for the training of the final model.\nEvaluation Metric $$\\begin{align} MSE \u0026amp;= \\frac{1}{N} \\sum_{i=1}^N (I_{HR}(i) - I_{SR}(i))^2 \\\\\\ PSNR \u0026amp;= 10 \\cdot \\log_{10}(\\frac{L^2}{MSE}) \\end{align}$$\nFor this challenge, the evaluation metric used is the Peak Signal-to-Noise Ratio (PSNR) score. PSNR is inversely proportional to the logarithm of the Mean Squared Error (MSE) between the ground truth image and the generated image.\nResults My final model uses MSRResNet (Modified SRResNet) with 20 residual blocks has a total of 1,812,995 parameters, trained using standard geometric augmentations, RGB permutation, Blend and MixUp augmentations obtained a PSNR score of 28.99 in Mini-DIV2k test set after 2 million training iterations. This modified version of SRResNet uses residual blocks without Batch Normalization, which is an enhancement suggested in the EDSR paper. The MSRResNet was configured to use 20 residual blocks instead of the original 16 blocks to increase its depth and it is strictly within the trainable parameters of 1,821,085.\nNote that the final grades for this challenge was not released as it was a final project, but it can be assumed that my submission was one of the top-performing candidates based on my final results for the course.\nEnd Notes Here\u0026rsquo;s a list of other techniques I\u0026rsquo;ve tried, but did not improve my model:\nSome data augmentations like: CutBlur Random brightness and contrast Other model architectures such as: EDSR CARN CARN Architecture The CARN model is based on ResNet architecture like the baseline, SRResNet. The main difference between CARN and ResNet is the presence of local and global cascading modules. In the CARN model, each residual block is changed to a “local” cascading block and the blue arrows indicate “global” cascading connections. The outputs of intermediary layers are cascaded into the higher layers, and finally converge on a single 1×1 convolution layer.\n","date":1605571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613548093,"objectID":"34fad8148306882ec6b17c104332072e","permalink":"https://kkaryl.github.io/project/div2ksisr/","publishdate":"2020-11-17T00:00:00Z","relpermalink":"/project/div2ksisr/","section":"project","summary":"The goal of this challenge is to build a high performing light-weight neural network model that can increase the resolution of a single image (by four times).","tags":["Computer Vision","Image Super Resolution","Deep Learning","CNN","DIV2K","SRResNet","MixUp Training","CARN","Data Augmentations"],"title":"DIV2K Single Image Super-Resolution Challenge","type":"project"},{"authors":null,"categories":null,"content":"Foreword This was a memorable project for me as I had the opportunity to seek and try out numerous advanced machine learning techniques to improve my classification model.\nMy final model was designed to use pre-trained ImageNet MobileNetV2 weights with 40 multi-head binary classifiers. To account for the dataset imbalance issue, MixUp training was performed and the Focal loss function was used.\nProblem Statement The objective of this challenge is to build a high performance multi-label classiﬁer for face attributes classiﬁcation using a given dataset. The evaluation set is released one week prior to submission deadline. The higher the prediction accuracy obtained for the private testset, the higher the grades for this challenge.\nThere are some constraints set for this challenge:\nModel should be trained strictly on the trainset of CelebA. Only ImageNet pre-trained models are allowed. Ensemble of models is not allowed. The deliverables of this challenge include:\nA short report in pdf format of not more than five A4 pages. Predictions of the test set in a text file. Source codes for training and testing the model. Dataset CelebA The dataset used is from CelebA which contains 200 thousand cropped and aligned faces. The dataset is split into 162,770 images for training, 19,867 images for validation and 19,962 images for testing. There are a total of 40 binary attributes (labels) for each image, with 1 indicating positive prediction, while -1 indicates negative prediction. These attributes are often used to describe a face and some examples include black hair, smiling, mustache. Given an aligned facial image, the classifier must output its prediction for all 40 attributes, making this a multi-label classification problem.\nExploratory Data Analysis (EDA) Prior to building the model, EDA was performed to better analyze the given dataset. The frequency distribution of the labels of each dataset split is plotted using matplotlib.\nThe plot shows the following observations:\nThe splits looked to be stratiﬁed sampled for train and validation sets. The test set has some attributes sampled more frequently (e.g. Wearing Lipstick, Wavy Hair and Big Lips). The full dataset is highly imbalanced with: Common attributes like No Beard and Young weighing above 70%. Less than 5% of rare attributes such as Mustache and Bald. Further EDA process also found the following issues:\nMultiple mislabelled images such as: Mislabelled males as females with Male set to -1. Mislabelled females with No_Beard set to -1. 131 duplicated images with 99 of them having different attribute annotations. Other things to note:\nSome attribute labels such as \u0026ldquo;Attractive\u0026rdquo; is rather subjective. Train set contains images with annotations, but without faces. See my codes on:\nCelebA EDA: Github CelebA Duplicate Check: Github Implementation Data Augmentation Data augmentation is an important step in many computer vision tasks due to the scarcity of labelled dataset. It helps to “increase” the training set by creating variations of the original images. This expanded dataset alleviates the problem of overﬁtting and allows the model to generalize better.\nFor the train set, the images are center cropped to 198 × 158 with 50% probability of horizontal ﬂip and 50% probability for aﬃne transformations (shift, scale, rotate), before RGB normalization and conversion to tensors. For the validation and test set, the images are center cropped to 198 × 158 before RGB normalization and conversion to tensor.\nSee my test codes on data augmentations at: Github\nMixUp Training As recommended by He et al. in their Bag of Tricks paper, MixUp training is an alternate form data augmentation technique. In the training loop, pairs of images in a batch are interpolated using a λ value sampled from a beta distribution of α = 0.2 (hyperparameter). The training loss is also computed using a weighted (λ) linear combination of the two losses calculated from the two labels of the mixed images.\nSee my test codes on MixUp training at: Github\nFocal loss $$\\begin{align} CE(p_t) \u0026amp;= -log(p_t) \\\\\\ FL(p_t) \u0026amp;= -\\alpha (1-p_t)^{\\gamma} log (p_t) \\end{align}$$\nDue to the class imbalance problem of the dataset stated in EDA, there are more easy positives like No Beard than hard positives like Gray Hair. As a result, the model might tend to be biased towards learning more representations of the data-dominated class. Hence, by using Focal Loss (Equation 2) over Cross Entropy Loss (Equation 1), it can help the network to learn sparse hard examples better. The hyperparameter α controls the weights of positive and negative samples, while γ adjusts the rate at which easy examples are downweighted. After some empirical tuning, α = 0.25 and γ = 3 were set.\nResults My best model was trained using pre-trained ImageNet MobileNetV2 as backbone with 40 binary classifiers for identifying the attributes. It attained average accuracy of 91.55%, 92.28% and 91.71% on CelebA\u0026rsquo;s train, validation and test set respectively.\nAfter the release of the grades, the source of the private test set was announced to be from lfwA+. My submission obtained a grade of 25/25.\nSee my training notebook at: Github\nSee my inference notebook at: Github\nEnd Notes Here\u0026rsquo;s a list of other techniques I\u0026rsquo;ve tried, but did not improve my model:\nSome data augmentations like: ColorJitter FancyPCA Random brightness and contrast Adding of random Gaussian noise Random Erasing did not work well with MixUp training Label Smoothing Other backbone models like: ResNet50, ResNeXt50 Random Erasing Random Erasing is like dropout at input level. It cuts out randomly sized areas from random locations of the images with random probability. Much like image impainting, it makes it harder for the classifer to learn by hiding certain semantics from it.\nLabel Smoothing Label smoothing is a regularization technique by Szegedy et al., which perturbs the target variables, to make the model less conﬁdent of its predictions. It does so by performing a weighted linear combination of predicted values by an $\\epsilon$ value e.g. 0.1. It usually performs well on imbalanced datasets.\n","date":1603324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613548093,"objectID":"2a0a4a296d2aa95a2b2b22cb33798ae3","permalink":"https://kkaryl.github.io/project/celeba/","publishdate":"2020-10-22T00:00:00Z","relpermalink":"/project/celeba/","section":"project","summary":"The goal of this challenge is to build a high performing multi-label classiﬁer to identify the facial attributes in photographs.","tags":["Computer Vision","Deep Learning","CNN","EDA","CelebA","MobileNetV2","MixUp Training","Focal loss","Data Augmentations","Random Erasing","Label Smoothing"],"title":"CelebA Facial Attribute Recognition Challenge","type":"project"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\rMath In-line math: $x + y = z$\nBlock math:\n$$\n$$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\rPress the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\rCustom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\rQuestions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610677023,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://kkaryl.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Karyl Ong Jia Hui"],"categories":["Demo"],"content":"Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It\u0026rsquo;s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more The template is mobile first with a responsive design to ensure that your site looks stunning on every device. Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Guide and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy\u0026rsquo;s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Wowchemy Admin: An admin tool to automatically import publications from BibTeX Inspiration Check out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610674036,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://kkaryl.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"}]