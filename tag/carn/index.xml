<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CARN | Karyl | Homepage</title>
    <link>https://kkaryl.github.io/tag/carn/</link>
      <atom:link href="https://kkaryl.github.io/tag/carn/index.xml" rel="self" type="application/rss+xml" />
    <description>CARN</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2021</copyright><lastBuildDate>Tue, 17 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://kkaryl.github.io/images/icon_hu63be5c3ffb57da62f326d16fcb778481_27207_512x512_fill_lanczos_center_3.png</url>
      <title>CARN</title>
      <link>https://kkaryl.github.io/tag/carn/</link>
    </image>
    
    <item>
      <title>DIV2K Single Image Super-Resolution Challenge</title>
      <link>https://kkaryl.github.io/project/div2ksisr/</link>
      <pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://kkaryl.github.io/project/div2ksisr/</guid>
      <description>&lt;h2 id=&#34;foreword&#34;&gt;Foreword&lt;/h2&gt;
&lt;p&gt;Single image super-resolution (SISR) aims to recover a high-resolution (HR) image from its low-resolution(LR) observation. There are two major challenges to project namely the constraint on the number of trainable parameters of the model and the long training times due to the large sizes of the HR images.&lt;/p&gt;
&lt;p&gt;The former constraint restricts the use of GANs, which is known to produce higher fidelity images as additional discriminator network will definitely exceed the number of trainable parameters. Due to limited computational resources, training a SISR model on a shared GPU resource provided by the institution took too much time per training iteration. This problem was alleviated by converting the raw images into subimages and stored in lmdb format, which allowed much faster disk IO. However, it will still take an entire day to train 1 million iterations. This limits the amount of empirical tuning during the model training and thus forces students to seek out the most efficient and effective methods to try.&lt;/p&gt;
&lt;p&gt;My final model was designed to use a modified version of SRResNet (MSRResNet) trained on a mini-DIV2K dataset with numerous data augmentations such as MixUp training and L1 loss. It achieved a PSNR score on 28.99 on the validation set after 2M iterations.&lt;/p&gt;
&lt;h2 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h2&gt;
&lt;p&gt;The objective of this challenge is to build a high performing light-weight neural network model that can increase the resolution of a single image (by four times) using a given dataset.  The evaluation set (80 test images) is released one week prior to submission deadline. The higher the PSNR obtained for the private testset, the higher the grades for this challenge.&lt;/p&gt;
&lt;p&gt;There are some &lt;strong&gt;constraints&lt;/strong&gt; set for this challenge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Model should be trained strictly on the provided mini DIV2K training set.&lt;/li&gt;
&lt;li&gt;Model should contain fewer than 1,821,085 trainable parameters, which is 120% of the trainable parameters in SRResNet.&lt;/li&gt;
&lt;li&gt;No external data and pretrained models are allowed.&lt;/li&gt;
&lt;li&gt;Ensemble of models is not allowed.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;strong&gt;deliverables&lt;/strong&gt; of this challenge include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A short report in pdf format of not more than five A4 pages.
&lt;ul&gt;
&lt;li&gt;PSNR of model on validation sets&lt;/li&gt;
&lt;li&gt;Number of parameters of final model&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The four times upscaled images of the 80 test LR images.&lt;/li&gt;
&lt;li&gt;Model checkpoint (weights) of submitted model.&lt;/li&gt;
&lt;li&gt;Source codes for training and testing the model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;dataset&#34;&gt;Dataset&lt;/h2&gt;
&lt;h3 id=&#34;mini-div2k&#34;&gt;Mini DIV2K&lt;/h3&gt;
&lt;p&gt;The mini dataset provided was extracted from the larger &lt;a href=&#34;https://data.vision.ee.ethz.ch/cvl/DIV2K/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DIV2K&lt;/a&gt; dataset, which originally contains 800 HR and corresponding low resolution images for training and 100 HR and LR images for validation. In this project, the mini DIV2K dataset contains 500 HR and LRx4 images for training and 80 pairs for validation and will be hereafter denoted as DIV2K for simplicity.&lt;/p&gt;
&lt;h3 id=&#34;dataset-preparation&#34;&gt;Dataset Preparation&lt;/h3&gt;
&lt;p&gt;As the DIV2K training dataset contains large 2K images, it takes a long time to load the HR images into memory for training. In order to improve the speed of disk IO during training, the 500 HR images are first cropped into 20,424 of 480x480 subimages before converting into a lmdb dataset (HRsub.lmdb) format. Similarly, the 500 corresponding LR images are also cropped into 20,424 of 120x120 subimages before converting to a lmdb dataset (LRx4sub.lmdb).&lt;/p&gt;
&lt;p&gt;One thing to note is the disadvantage for converting the images into lmdb format. The two lmdb files required close to 70 GB of disk space as compared to the original 3GB of all the image files. However, it is a necessary tradeoff to train using the lmdb files as the training speed was increased at least 2 folds.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;h3 id=&#34;data-augmentation&#34;&gt;Data Augmentation&lt;/h3&gt;
&lt;p&gt;Data augmentation is an important step in many computer vision tasks due to the scarcity of labelled dataset. It helps to “increase” the training set by creating variations of the original images. This expanded dataset alleviates the problem of overﬁtting and allows the model to generalize better.&lt;/p&gt;
&lt;p&gt;Standard geometric manipulation techniques such as paired random crop, random horizontal and vertical flips and random rotations, provided by the baseline BasicSR are applied to the model training.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;SISR_data_aug.png&#34; alt=&#34;Non-standard Data Augmentations&#34;&gt;&lt;/p&gt;
&lt;p&gt;Inspired by Yoo et al. findings on effective augmentations for Image Super Resolution, augmentations such as RGB permutations and Blending, which performs no structural changes to the images were applied. RGB permutation is the random shuffling of the color channels of the HR and LR images (same permutations), while Blending adds a random constant value to the images. As experimented, these two augmentation improves the PSNR by 0.03 on the validation set. Futhermore, MixUp training was also performed. MixUp is a data augmentation technique that blends two images to generate an unseen training sample. As such pairs of LR and HR images are mixed together using a weighted linear interpolation with a λ value sampled from a beta distribution of α = 0.2 (hyperparameter).&lt;/p&gt;
&lt;h3 id=&#34;l1-loss&#34;&gt;L1 loss&lt;/h3&gt;
&lt;p&gt;$$\begin{align}
L1 &amp;amp;= \frac{1}{N} \sum_{i=1}^N |I_{HR}(i) - I_{SR}(i)|
\end{align}$$&lt;/p&gt;
&lt;p&gt;As the PSNR metric is highly correlated with the pixel-wise difference, and minimizing the pixel loss directly maximizes the PSNR metric value. It was observed that L1 Loss performs better than MSE Loss in terms of PSNR on validation set, thus it was used for the training of the final model.&lt;/p&gt;
&lt;h3 id=&#34;evaluation-metric&#34;&gt;Evaluation Metric&lt;/h3&gt;
&lt;p&gt;$$\begin{align}
MSE &amp;amp;= \frac{1}{N} \sum_{i=1}^N (I_{HR}(i) - I_{SR}(i))^2 \\\
PSNR &amp;amp;= 10 \cdot \log_{10}(\frac{L^2}{MSE})
\end{align}$$&lt;/p&gt;
&lt;p&gt;For this challenge, the evaluation metric used is the Peak Signal-to-Noise Ratio (PSNR) score. PSNR is inversely proportional to the logarithm of the Mean Squared Error (MSE) between the ground truth image and the generated image.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;My final model uses MSRResNet (Modified SRResNet) with 20 residual blocks has a total of &lt;strong&gt;1,812,995&lt;/strong&gt; parameters, trained using standard geometric augmentations, RGB permutation, Blend and MixUp augmentations obtained a PSNR score of &lt;strong&gt;28.99&lt;/strong&gt; in Mini-DIV2k test set after 2 million training iterations. This modified version of SRResNet uses residual blocks without Batch Normalization, which is an enhancement suggested in the EDSR paper. The MSRResNet was configured to use 20 residual blocks instead of the original 16 blocks to increase its depth and it is strictly within the trainable parameters of 1,821,085.&lt;/p&gt;
&lt;p&gt;Note that the final grades for this challenge was not released as it was a final project, but it can be assumed that my submission was one of the top-performing candidates based on my final results for the course.&lt;/p&gt;
&lt;!-- See my training notebook at: &lt;a href=&#34;https://github.com/kkaryl/AI6126-Advanced_Computer_Vision/blob/master/project_1/src/ai6126-p1-train-v1.5.ipynb&#34;&gt;&lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt; Github&lt;/a&gt;

See my inference notebook at: &lt;a href=&#34;https://github.com/kkaryl/AI6126-Advanced_Computer_Vision/blob/master/project_1/src/ai6126-p1-inference-v0.3.ipynb&#34;&gt;&lt;i class=&#34;fab fa-github&#34;&gt;&lt;/i&gt; Github&lt;/a&gt; --&gt;
&lt;h2 id=&#34;end-notes&#34;&gt;End Notes&lt;/h2&gt;
&lt;p&gt;Here&amp;rsquo;s a list of other techniques I&amp;rsquo;ve tried, but did not improve my model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some data augmentations like:
&lt;ul&gt;
&lt;li&gt;CutBlur&lt;/li&gt;
&lt;li&gt;Random brightness and contrast&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Other model architectures such as:
&lt;ul&gt;
&lt;li&gt;EDSR&lt;/li&gt;
&lt;li&gt;CARN&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;carn-architecture&#34;&gt;CARN Architecture&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;CARN.png&#34; alt=&#34;CARN Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The CARN model is based on ResNet architecture like the baseline, SRResNet. The main difference between CARN and ResNet is the presence of local and global cascading modules.  In the CARN model, each residual block is changed to a “local” cascading block and the blue arrows indicate “global” cascading connections. The outputs of intermediary layers are cascaded into the higher layers, and finally converge on a single 1×1 convolution layer.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
